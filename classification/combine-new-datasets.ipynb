{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mwsb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mwsb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mwsb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mwsb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to /home/mwsb/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, csv\n",
    "import gc\n",
    "\n",
    "from pprint import pprint\n",
    "from langdetect import detect\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'http', 'javascript'])\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detect(ls):\n",
    "    text = ' '.join(ls)\n",
    "    result = detect(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text))\n",
    "\n",
    "def preprocess(sentences):\n",
    "    result = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        lemmas = []\n",
    "        # tokenize\n",
    "        tokens = gensim.utils.simple_preprocess(sent)\n",
    "        for token in tokens:\n",
    "            lemma = lemmatize_stemming(token)\n",
    "            # remove stopwords\n",
    "            if lemma not in stop_words:\n",
    "                lemmas.append(lemma)\n",
    "#         # POS tagging\n",
    "#         nltk_tagged = nltk.pos_tag(lemmas)\n",
    "#         for word, tag in nltk_tagged:\n",
    "#             # only keep nouns\n",
    "#             if tag.startswith('N'):\n",
    "#                 result.append(word)\n",
    "        # Without POS tagging\n",
    "        result += lemmas\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts in the list\n",
    "def preprocess_texts(ls):\n",
    "    parsed_ls = []\n",
    "    if len(ls) > 0:\n",
    "        for i in range(len(ls)):\n",
    "            txt = str(ls[i])\n",
    "            # Only include non-empty sentences with length > 1\n",
    "            if len(txt) > 1 and txt != ' ':\n",
    "                txt = remove_sp_char(txt)\n",
    "                txt = remove_links(txt)\n",
    "                txt = remove_email(txt)\n",
    "                txt = remove_single_char(txt)\n",
    "                txt = remove_multi_spaces(txt)\n",
    "                txt = txt.strip()\n",
    "                if len(txt) > 1:\n",
    "                    txt = remove_noneng(txt)\n",
    "                    if txt != '':\n",
    "                        parsed_ls.append(txt)\n",
    "        return parsed_ls\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi_spaces(text):\n",
    "    return re.sub(' +', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_char(text):\n",
    "    return re.sub('(^| ).( |$)', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters in the text\n",
    "def remove_sp_char(text):\n",
    "    return re.sub('[^0-9a-zA-Z]+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URL links in the text\n",
    "def remove_links(text):\n",
    "    text = re.sub('(?:https?://)?(?:www)?(\\S*?\\.onion)\\b', '', text)\n",
    "    text = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove email addressed in the text\n",
    "def remove_email(text):\n",
    "    return re.sub('\\S*@\\S*\\s?', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-english sentences with ''\n",
    "def remove_noneng(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang != 'en':\n",
    "            return ''\n",
    "        else:\n",
    "            return text\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(url):\n",
    "    url = str(url)\n",
    "    result = re.findall('^(?:https?\\:\\/\\/)?[\\w\\-\\.]+\\.onion', url)\n",
    "    if len(result) < 1:\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categories ##\n",
    "# 1: Market: Drug, gun, \n",
    "# 2: Counterfeit: counterfeit credit cards, money, ID\n",
    "# 3: Services: Hosting service, forum, email, pastebin, file-sharing\n",
    "# 4: Security: Security-related information, tutorials or services, leaked data\n",
    "# 5: Porn: Hosting pornographic material\n",
    "# 6: Cryptocurrency\n",
    "# 7: NoAccess: Login, Down, Empty\n",
    "# 8: Other: Cannot be classified in any other category (e.g. personal blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of conversion of categories\n",
    "conversion_dict = {\n",
    "    'Art': 'Other',\n",
    "    'Casino': 'Services',\n",
    "    'Counterfeit Credit-Cards': 'Counterfeit',\n",
    "    'Counterfeit Money': 'Counterfeit',\n",
    "    'Counterfeit Personal-Identification': 'Counterfeit',\n",
    "    'Cryptocurrency': 'Cryptocurrency',\n",
    "    'Cryptolocker': 'Security',\n",
    "    'Down': 'NoAccess',\n",
    "    'Drugs': 'Market',\n",
    "    'Empty': 'NoAccess',\n",
    "    'Forum': 'Services',\n",
    "    'Hacking': 'Security',\n",
    "    'Hosting': 'Services',\n",
    "    'Leaked-Data': 'Services',\n",
    "    'Library': 'Other',\n",
    "    'Locked': 'NoAccess',\n",
    "    'Marketplace': 'Market',\n",
    "#     'Onion Directory/Wiki': ['directory', 'dir', 'wiki'],\n",
    "    'Personal': 'Other',\n",
    "    'Politics': 'Other',\n",
    "    'Porno': 'Porn',\n",
    "    'Religion': 'Other',\n",
    "    'Services': 'Services',\n",
    "    'Social-Network': 'Services',\n",
    "    'Violence': 'Market',\n",
    "    'Other': 'Other'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data\n",
    "model_training_dataset_domain = pd.read_csv('../data/model_training_dataset_domain.csv')\n",
    "model_training_dataset_domain['body_text'] = model_training_dataset_domain['body_text'].apply(ast.literal_eval)\n",
    "existing_domains = list(model_training_dataset_domain['domain_url'])\n",
    "del model_training_dataset_domain\n",
    "gc.collect()\n",
    "\n",
    "datasets = []\n",
    "dates = ['jan-15-0', 'jan-15-1', 'feb-19-0', 'feb-19-1']\n",
    "for date in dates:\n",
    "    df = pd.read_csv('../data/dataset-'+date+'.csv')\n",
    "    df['body_text'] = df['body_text'].apply(ast.literal_eval)\n",
    "    datasets.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22222222n77jskuw.onion</td>\n",
       "      <td>Porn Videos - XONIONS - THE BEST ONION PORN SI...</td>\n",
       "      <td>[Amateur anal, 5, 7 minMomcikoper -  292.3k Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22222223nm4siaje.onion</td>\n",
       "      <td>Porn Videos - XONIONS - THE BEST ONION PORN SI...</td>\n",
       "      <td>[Amateur anal, 5, 7 minMomcikoper -  292.3k Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222222afubjlbhm.onion</td>\n",
       "      <td>Porn Videos - XONIONS - THE BEST ONION PORN SI...</td>\n",
       "      <td>[Amateur anal, 5, 7 minMomcikoper -  292.3k Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22oxht5ep3hvyboc.onion</td>\n",
       "      <td>Onion Dir - Adult</td>\n",
       "      <td>[http://bitcoi6dzn2c24oa.onion, CP - Teens - J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22pp2nrnjcmtlzja.onion</td>\n",
       "      <td>PayPal Plaza | The Tor Marketplace For Buying ...</td>\n",
       "      <td>[Order Now ($76.12), $721.60, Beware Of Fake S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16408</th>\n",
       "      <td>oscarn4se6ji4leq.onion</td>\n",
       "      <td>Stolen Credit Card Informations</td>\n",
       "      <td>[ContactOscar, Privacy Policy, United States C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16409</th>\n",
       "      <td>oscaroo67vmbwsf3.onion</td>\n",
       "      <td>BANK ACCOUNT - 1000$ - BANK OF AMERICA</td>\n",
       "      <td>[Warning: unlink(/var/www/oscars/oscar2_storag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>oscarw4be4xs2pct.onion</td>\n",
       "      <td>BANK ACCOUNT - 1000$ - BANK OF AMERICA</td>\n",
       "      <td>[Credit Cards (CC's)11HomeAccounts(BANK,PAYPAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16411</th>\n",
       "      <td>satforumnoo6sxgk.onion</td>\n",
       "      <td>download archives</td>\n",
       "      <td>[6 \\t qX`F!A��–V� ����`J��P�#H�u._���k]�:S�p7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16412</th>\n",
       "      <td>supportvojao2z6taveolgpvgcz5k4v7smwgjcuzz5ahs5...</td>\n",
       "      <td>Pedo Support Community 6.0 - Index</td>\n",
       "      <td>[1 Replies, Pedo Support Community 6.0 - Index...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              domain_url  \\\n",
       "0                                 22222222n77jskuw.onion   \n",
       "1                                 22222223nm4siaje.onion   \n",
       "2                                 2222222afubjlbhm.onion   \n",
       "3                                 22oxht5ep3hvyboc.onion   \n",
       "4                                 22pp2nrnjcmtlzja.onion   \n",
       "...                                                  ...   \n",
       "16408                             oscarn4se6ji4leq.onion   \n",
       "16409                             oscaroo67vmbwsf3.onion   \n",
       "16410                             oscarw4be4xs2pct.onion   \n",
       "16411                             satforumnoo6sxgk.onion   \n",
       "16412  supportvojao2z6taveolgpvgcz5k4v7smwgjcuzz5ahs5...   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Porn Videos - XONIONS - THE BEST ONION PORN SI...   \n",
       "1      Porn Videos - XONIONS - THE BEST ONION PORN SI...   \n",
       "2      Porn Videos - XONIONS - THE BEST ONION PORN SI...   \n",
       "3                                      Onion Dir - Adult   \n",
       "4      PayPal Plaza | The Tor Marketplace For Buying ...   \n",
       "...                                                  ...   \n",
       "16408                    Stolen Credit Card Informations   \n",
       "16409             BANK ACCOUNT - 1000$ - BANK OF AMERICA   \n",
       "16410             BANK ACCOUNT - 1000$ - BANK OF AMERICA   \n",
       "16411                                  download archives   \n",
       "16412                 Pedo Support Community 6.0 - Index   \n",
       "\n",
       "                                               body_text  \n",
       "0      [Amateur anal, 5, 7 minMomcikoper -  292.3k Vi...  \n",
       "1      [Amateur anal, 5, 7 minMomcikoper -  292.3k Vi...  \n",
       "2      [Amateur anal, 5, 7 minMomcikoper -  292.3k Vi...  \n",
       "3      [http://bitcoi6dzn2c24oa.onion, CP - Teens - J...  \n",
       "4      [Order Now ($76.12), $721.60, Beware Of Fake S...  \n",
       "...                                                  ...  \n",
       "16408  [ContactOscar, Privacy Policy, United States C...  \n",
       "16409  [Warning: unlink(/var/www/oscars/oscar2_storag...  \n",
       "16410  [Credit Cards (CC's)11HomeAccounts(BANK,PAYPAL...  \n",
       "16411  [6 \\t qX`F!A��–V� ����`J��P�#H�u._���k]�:S�p7...  \n",
       "16412  [1 Replies, Pedo Support Community 6.0 - Index...  \n",
       "\n",
       "[16413 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22oxht5ep3hvyboc.onion</td>\n",
       "      <td>Onion Dir - Adult</td>\n",
       "      <td>[Baby Bitch CP is an unique new portal. We are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22pp2nrnjcmtlzja.onion</td>\n",
       "      <td>PayPal Plaza | The Tor Marketplace For Buying ...</td>\n",
       "      <td>[$47.02, Order Now ($11.05), $75.09, What You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2464b3fu462tx2en.onion</td>\n",
       "      <td>CHILD PORN CENTER</td>\n",
       "      <td>[6, CHILD PORN CENTER, http://x5y2b4xjf46idxzp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24adsavjl3u6tylr.onion</td>\n",
       "      <td>CP Video HD 2021, PTHC, Loli, child porn, pret...</td>\n",
       "      <td>[70 Gb, Video HD 4K, 400 Gb, CC, © Copyright 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ajtkan56n6aiyl6.onion</td>\n",
       "      <td>Hard Porn Forum</td>\n",
       "      <td>[Category, 36, Email, value 01, 268, 78, video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12551</th>\n",
       "      <td>rkvwooasau2goqeamkqbade4yv2hnt6dc4ol5kgnaaoihd...</td>\n",
       "      <td>Moneys spider - financial pyramid</td>\n",
       "      <td>[Create account, Email address, No wallet? get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12552</th>\n",
       "      <td>e26whn2524322mkxb3cbyk27ev2ihhq2biz35hty7gzgsy...</td>\n",
       "      <td>BenTasker.co.uk - The Home of Ben Tasker - www...</td>\n",
       "      <td>[Home / Uncategorised, Latest Posts, This site...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12553</th>\n",
       "      <td>4vzhn5j644aa523g.onion</td>\n",
       "      <td>Canny</td>\n",
       "      <td>[4vzhn5j644aa523g.onion, Buy the access to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12554</th>\n",
       "      <td>cashgodr53umth4z.onion</td>\n",
       "      <td>Cash God - Real Cash Sellers</td>\n",
       "      <td>[No Risk for You, Our Method, Contact us, 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12555</th>\n",
       "      <td>okhvh6c75sbxgre5.onion</td>\n",
       "      <td>PORN</td>\n",
       "      <td>[Login, Sign up, PORN, Don’t have an account?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              domain_url  \\\n",
       "0                                 22oxht5ep3hvyboc.onion   \n",
       "1                                 22pp2nrnjcmtlzja.onion   \n",
       "2                                 2464b3fu462tx2en.onion   \n",
       "3                                 24adsavjl3u6tylr.onion   \n",
       "4                                 2ajtkan56n6aiyl6.onion   \n",
       "...                                                  ...   \n",
       "12551  rkvwooasau2goqeamkqbade4yv2hnt6dc4ol5kgnaaoihd...   \n",
       "12552  e26whn2524322mkxb3cbyk27ev2ihhq2biz35hty7gzgsy...   \n",
       "12553                             4vzhn5j644aa523g.onion   \n",
       "12554                             cashgodr53umth4z.onion   \n",
       "12555                             okhvh6c75sbxgre5.onion   \n",
       "\n",
       "                                                   title  \\\n",
       "0                                      Onion Dir - Adult   \n",
       "1      PayPal Plaza | The Tor Marketplace For Buying ...   \n",
       "2                                      CHILD PORN CENTER   \n",
       "3      CP Video HD 2021, PTHC, Loli, child porn, pret...   \n",
       "4                                        Hard Porn Forum   \n",
       "...                                                  ...   \n",
       "12551                  Moneys spider - financial pyramid   \n",
       "12552  BenTasker.co.uk - The Home of Ben Tasker - www...   \n",
       "12553                                              Canny   \n",
       "12554                       Cash God - Real Cash Sellers   \n",
       "12555                                               PORN   \n",
       "\n",
       "                                               body_text  \n",
       "0      [Baby Bitch CP is an unique new portal. We are...  \n",
       "1      [$47.02, Order Now ($11.05), $75.09, What You ...  \n",
       "2      [6, CHILD PORN CENTER, http://x5y2b4xjf46idxzp...  \n",
       "3      [70 Gb, Video HD 4K, 400 Gb, CC, © Copyright 2...  \n",
       "4      [Category, 36, Email, value 01, 268, 78, video...  \n",
       "...                                                  ...  \n",
       "12551  [Create account, Email address, No wallet? get...  \n",
       "12552  [Home / Uncategorised, Latest Posts, This site...  \n",
       "12553  [4vzhn5j644aa523g.onion, Buy the access to the...  \n",
       "12554  [No Risk for You, Our Method, Contact us, 2020...  \n",
       "12555     [Login, Sign up, PORN, Don’t have an account?]  \n",
       "\n",
       "[12556 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.concat(datasets)\n",
    "df_combined.reset_index(inplace=True)\n",
    "df_combined.drop(columns=['index'], inplace=True)\n",
    "# Remove duplicated domains\n",
    "df_combined = df_combined.drop_duplicates(subset ='domain_url', keep = 'first')\n",
    "df_combined.reset_index(inplace=True)\n",
    "df_combined.drop(columns=['index'], inplace=True)\n",
    "# Remove existing domains in trainset\n",
    "df_combined = df_combined[~df_combined['domain_url'].isin(existing_domains)]\n",
    "df_combined.reset_index(inplace=True)\n",
    "df_combined.drop(columns=['index'], inplace=True)\n",
    "# Only keep unique sentences\n",
    "df_combined['body_text'] = df_combined['body_text'].apply(lambda x: list(set(x)))\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('../data/dataset-combined-jan-feb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
