{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from bs4 import BeautifulSoup\n",
    "from json_stream_parser import load_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert html to string\n",
    "def body2text_body(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    result = []\n",
    "    # kill all script and style elements\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            result.append(line)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwsb/.local/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://xu5dw5izkd5acb2z66vkbh2hgkxajtyvj7txg7sttpozpk5mx7nuqcyd.onion/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/mwsb/.local/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://u2izrmre7yqzhv5gr3tbbsoyriwoxmc77rfieuz7xdjzjpvk37bhahqd.onion/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/mwsb/.local/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://az67bzyxmctkg4mgpob7pdi54ea3tftrnpcvdx6owhdrnvdiqtl4s7id.onion/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/mwsb/.local/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://jgwk27t33tmvf4zigenw6xcjauvipma2b5o2mxgf2xhk5oo3xvjilqid.onion/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/mwsb/.local/lib/python3.7/site-packages/bs4/__init__.py:282: UserWarning: \"http://dark5rc4wrxftqix.onion/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output csv 0\n",
      "Output csv 1\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 500000\n",
    "i = 0\n",
    "data = []\n",
    "date = 'jan-15'\n",
    "with open('../data/universe-' + date + '.json') as data_file:\n",
    "    for obj in load_iter(data_file):\n",
    "        body = obj['_source']['body']\n",
    "        body_text = body2text_body(body)\n",
    "        row = [obj['_source']['url'], obj['_source']['title'], body_text, obj['_source']['status']]\n",
    "        data.append(row)\n",
    "        if len(data) >= LIMIT:\n",
    "            print('Output csv ' + str(i))\n",
    "            df = pd.DataFrame(data, columns=['url', 'title', 'body_text', 'status'])\n",
    "            df.to_csv('../data/universe-'+date+'-'+str(i)+'.csv', index=False)\n",
    "            del df\n",
    "            data = []\n",
    "            i += 1\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
