{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, csv\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from operator import add\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "# from xgboost import XGBClassifier\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from transformers import AlbertModel, AlbertTokenizer, AlbertForSequenceClassification, AlbertConfig\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categories ##\n",
    "# Market: Drug, gun, \n",
    "# Counterfeit: counterfeit credit cards, money, ID\n",
    "# Services: Hosting service, forum, email, pastebin, file-sharing\n",
    "# Security: Security-related information, tutorials or services, leaked data\n",
    "# Porn: Hosting pornographic material\n",
    "# Cryptocurrency\n",
    "# NoAccess: Login, Down, Empty\n",
    "# Other: Cannot be classified in any other category (e.g. personal blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of conversion of categories\n",
    "conversion_dict = {\n",
    "    'Art': 'Other',\n",
    "    'Casino': 'Services',\n",
    "    'Counterfeit Credit-Cards': 'Counterfeit',\n",
    "    'Counterfeit Money': 'Counterfeit',\n",
    "    'Counterfeit Personal-Identification': 'Counterfeit',\n",
    "    'Cryptocurrency': 'Cryptocurrency',\n",
    "    'Cryptolocker': 'Security',\n",
    "    'Down': 'NoAccess',\n",
    "    'Drugs': 'Market',\n",
    "    'Empty': 'NoAccess',\n",
    "    'Forum': 'Services',\n",
    "    'Hacking': 'Security',\n",
    "    'Hosting': 'Services',\n",
    "    'Leaked-Data': 'Security',\n",
    "    'Library': 'Other',\n",
    "    'Locked': 'NoAccess',\n",
    "    'Marketplace': 'Market',\n",
    "#     'Onion Directory/Wiki': ['directory', 'dir', 'wiki'],\n",
    "    'Personal': 'Other',\n",
    "    'Politics': 'Other',\n",
    "    'Porno': 'Porn',\n",
    "    'Religion': 'Other',\n",
    "    'Services': 'Services',\n",
    "    'Social-Network': 'Services',\n",
    "    'Violence': 'Market'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data\n",
    "df_combined = pd.read_csv('../data/model_training_dataset.csv')\n",
    "\n",
    "# convert the string representation of a list into a list\n",
    "df_combined['url'] = df_combined['url'].apply(ast.literal_eval)\n",
    "df_combined['body_text'] = df_combined['body_text'].apply(ast.literal_eval)\n",
    "df_combined['body_token'] = df_combined['body_token'].apply(ast.literal_eval)\n",
    "df_combined['class'] = df_combined['class'].apply(lambda x: x.strip())\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_token</th>\n",
       "      <th>language</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[wwjewp6ca4rkudyj.onion]</td>\n",
       "      <td>\\n  Frënn vun der Ënn - Index\\n</td>\n",
       "      <td>[Frënn vun der Ënn - Index, Skip to main conte...</td>\n",
       "      <td>[frënn, vun, der, ënn, index, skip, content, e...</td>\n",
       "      <td>en</td>\n",
       "      <td>Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[u76xgym22s4adf55.onion]</td>\n",
       "      <td>PayPal Master: Home</td>\n",
       "      <td>[PayPal Master: Home, Contact us at: ppmaster@...</td>\n",
       "      <td>[master, home, contact, ppmaster, deepdarkmail...</td>\n",
       "      <td>en</td>\n",
       "      <td>Counterfeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2hftxvyft7dl3fk2.onion]</td>\n",
       "      <td>Umbrella Escrow</td>\n",
       "      <td>[Umbrella Escrow, Start new escrow, View exist...</td>\n",
       "      <td>[umbrella, escrow, escrow, view, escrow, instr...</td>\n",
       "      <td>en</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[elherbotsiddarol.onion]</td>\n",
       "      <td>ElHerbolario</td>\n",
       "      <td>[ElHerbolario, My Account, Blog, PGP, Sitemap,...</td>\n",
       "      <td>[elherbolario, account, blog, pgp, sitemap, nb...</td>\n",
       "      <td>en</td>\n",
       "      <td>Market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[yermrrzeg4fusqx5.onion]</td>\n",
       "      <td>anonymous OnionMail Server</td>\n",
       "      <td>[anonymous OnionMail Server, anonymous OnionMa...</td>\n",
       "      <td>[onionmail, server, onionmail, serveronion, ma...</td>\n",
       "      <td>en</td>\n",
       "      <td>Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>[5wkkhgqtity7bhdf.onion]</td>\n",
       "      <td>walletGenerator - Universal Paper wallet gener...</td>\n",
       "      <td>[walletGenerator - Universal Paper wallet gene...</td>\n",
       "      <td>[walletgener, univers, paper, wallet, gener, b...</td>\n",
       "      <td>en</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>[ctzzqqimlfamyhrc.onion]</td>\n",
       "      <td>| TheYOSH.nl</td>\n",
       "      <td>[| TheYOSH.nl, Skip to main content, User acco...</td>\n",
       "      <td>[theyosh, nl, skip, content, user, account, me...</td>\n",
       "      <td>en</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>[grrmailb3fxpjbwm.onion]</td>\n",
       "      <td>✉ Guerrilla Mail on Tor</td>\n",
       "      <td>[✉ Guerrilla Mail on Tor, TorGuerrillaMail - D...</td>\n",
       "      <td>[guerrilla, mail, tor, dispos, mail, address, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>[greenegbqkyk3ois.onion]</td>\n",
       "      <td>✔ Legit Carding Services ♛ Money Transfer Worl...</td>\n",
       "      <td>[✔ Legit Carding Services ♛ Money Transfer Wor...</td>\n",
       "      <td>[card, servic, money, transfer, worldwid, webs...</td>\n",
       "      <td>en</td>\n",
       "      <td>Counterfeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>[mldozg3tw4qyzavm.onion, mldozg3tw4qyzavm.onio...</td>\n",
       "      <td>��� ����� � ������� - The Front Page</td>\n",
       "      <td>[��� ����� � ������� - The Front Page, ������,...</td>\n",
       "      <td>[page, admin, cm, articl, onion, tor, browser,...</td>\n",
       "      <td>en</td>\n",
       "      <td>NoAccess</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1986 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0                              [wwjewp6ca4rkudyj.onion]   \n",
       "1                              [u76xgym22s4adf55.onion]   \n",
       "2                              [2hftxvyft7dl3fk2.onion]   \n",
       "3                              [elherbotsiddarol.onion]   \n",
       "4                              [yermrrzeg4fusqx5.onion]   \n",
       "...                                                 ...   \n",
       "1981                           [5wkkhgqtity7bhdf.onion]   \n",
       "1982                           [ctzzqqimlfamyhrc.onion]   \n",
       "1983                           [grrmailb3fxpjbwm.onion]   \n",
       "1984                           [greenegbqkyk3ois.onion]   \n",
       "1985  [mldozg3tw4qyzavm.onion, mldozg3tw4qyzavm.onio...   \n",
       "\n",
       "                                                  title  \\\n",
       "0                       \\n  Frënn vun der Ënn - Index\\n   \n",
       "1                                   PayPal Master: Home   \n",
       "2                                       Umbrella Escrow   \n",
       "3                                          ElHerbolario   \n",
       "4                            anonymous OnionMail Server   \n",
       "...                                                 ...   \n",
       "1981  walletGenerator - Universal Paper wallet gener...   \n",
       "1982                                       | TheYOSH.nl   \n",
       "1983                            ✉ Guerrilla Mail on Tor   \n",
       "1984  ✔ Legit Carding Services ♛ Money Transfer Worl...   \n",
       "1985               ��� ����� � ������� - The Front Page   \n",
       "\n",
       "                                              body_text  \\\n",
       "0     [Frënn vun der Ënn - Index, Skip to main conte...   \n",
       "1     [PayPal Master: Home, Contact us at: ppmaster@...   \n",
       "2     [Umbrella Escrow, Start new escrow, View exist...   \n",
       "3     [ElHerbolario, My Account, Blog, PGP, Sitemap,...   \n",
       "4     [anonymous OnionMail Server, anonymous OnionMa...   \n",
       "...                                                 ...   \n",
       "1981  [walletGenerator - Universal Paper wallet gene...   \n",
       "1982  [| TheYOSH.nl, Skip to main content, User acco...   \n",
       "1983  [✉ Guerrilla Mail on Tor, TorGuerrillaMail - D...   \n",
       "1984  [✔ Legit Carding Services ♛ Money Transfer Wor...   \n",
       "1985  [��� ����� � ������� - The Front Page, ������,...   \n",
       "\n",
       "                                             body_token language  \\\n",
       "0     [frënn, vun, der, ënn, index, skip, content, e...       en   \n",
       "1     [master, home, contact, ppmaster, deepdarkmail...       en   \n",
       "2     [umbrella, escrow, escrow, view, escrow, instr...       en   \n",
       "3     [elherbolario, account, blog, pgp, sitemap, nb...       en   \n",
       "4     [onionmail, server, onionmail, serveronion, ma...       en   \n",
       "...                                                 ...      ...   \n",
       "1981  [walletgener, univers, paper, wallet, gener, b...       en   \n",
       "1982  [theyosh, nl, skip, content, user, account, me...       en   \n",
       "1983  [guerrilla, mail, tor, dispos, mail, address, ...       en   \n",
       "1984  [card, servic, money, transfer, worldwid, webs...       en   \n",
       "1985  [page, admin, cm, articl, onion, tor, browser,...       en   \n",
       "\n",
       "               class  \n",
       "0           Services  \n",
       "1        Counterfeit  \n",
       "2     Cryptocurrency  \n",
       "3             Market  \n",
       "4           Services  \n",
       "...              ...  \n",
       "1981  Cryptocurrency  \n",
       "1982           Other  \n",
       "1983        Services  \n",
       "1984     Counterfeit  \n",
       "1985        NoAccess  \n",
       "\n",
       "[1986 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_eng = df_combined[df_combined['language'] == 'en']\n",
    "df_combined_eng.reset_index(inplace=True)\n",
    "df_combined_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Services</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Counterfeit</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NoAccess</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Market</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cryptocurrency</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Other</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Security</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Porn</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            class  title\n",
       "0        Services    574\n",
       "1     Counterfeit    530\n",
       "2        NoAccess    212\n",
       "3          Market    208\n",
       "4  Cryptocurrency    128\n",
       "5           Other    125\n",
       "6        Security     80\n",
       "7            Porn     70"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of clusters in each category\n",
    "df_combined_eng_count = df_combined_eng[['title', 'class']].groupby(['class']).count()\n",
    "df_combined_eng_count = df_combined_eng_count.sort_values(by=['title'], ascending=False)\n",
    "df_combined_eng_count.reset_index(inplace=True)\n",
    "df_combined_eng_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "# plt.bar(df_combined_eng_count['class'], df_combined_eng_count['title'])\n",
    "# plt.xlabel(\"Category\")\n",
    "# plt.ylabel(\"Number of Clusters\")\n",
    "# plt.title(\"Number of Clusters in Each Category\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,20))\n",
    "# plt.pie(df_combined_eng_count['title'], labels=df_combined_eng_count['class'], autopct='%1.1f%%')\n",
    "# plt.title(\"Propotion of Clusters in Each Category\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined.to_csv('../data/universe_sample_verified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of labels\n",
    "labels_list = df_combined_eng['class'].values\n",
    "# Encode L as integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels_list)\n",
    "labels = le.transform(labels_list)\n",
    "# Get the number of labels\n",
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49921\n",
      "1\n",
      "541.9889224572004\n"
     ]
    }
   ],
   "source": [
    "# List of original body texts\n",
    "texts = df_combined_eng['body_text'].values\n",
    "# Explore the lengths of body texts\n",
    "texts_len = [len(txt) for txt in texts]\n",
    "print(max(texts_len))\n",
    "print(min(texts_len))\n",
    "print(sum(texts_len)/len(texts_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7207\n",
      "1\n",
      "120.21752265861028\n"
     ]
    }
   ],
   "source": [
    "# List of body texts without duplicates\n",
    "texts_set = []\n",
    "for txt in texts:\n",
    "    txt = set(txt)\n",
    "    texts_set.append(txt)\n",
    "\n",
    "# Print the length\n",
    "texts_set_len = [len(txt) for txt in texts_set]\n",
    "print(max(texts_set_len))\n",
    "print(min(texts_set_len))\n",
    "print(sum(texts_set_len)/len(texts_set_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]Organization [SEP] Partners [SEP] Services [SEP] Join Us [SEP] Skip to main content [SEP] Abuse [SEP] Worldmap [SEP] Support [SEP] Donation history [SEP] Become a Member [SEP] Mirrors [SEP] About [SEP] Learn more about Tor [SEP] Mailing List [SEP] Frënn vun der Ënn - Index [SEP] Frënn vun der Ënn A.S.B.L. [SEP] @FrennVunDerEnn [SEP] Wiki [SEP] Adopt a Bridge [SEP] Twitter [SEP] Privacy is the right to a free mind! [SEP] Statistics [SEP] I support FVDE because even small organizations can contribute to protect fundamental civil rights. This noble goal can be pursued by making use of technology and putting knowledge to the service of users who want to maintain their privacy. [SEP] Georges, Teacher [SEP] Traffic [SEP] General [SEP] Enn.lu [SEP] CC-BY-NC-SA [SEP] News [SEP] Luxembourg based non-profit organization defending civil rights on the internet. [SEP] Frënn vun der Enn a.s.b.l. (R.C.S. Luxembourg F 9.478) [SEP] About Ënnstatus [SEP] Edward J. Snowden [SEP] Contact [SEP] By bouncing your communications around a distributed network of relays run by volunteers all around the world, it prevents somebody watching your Internet connection from learning what sites you visit, and it prevents the sites you visit from learning your physical location. [SEP] Tor is free software and an open network that helps you defend against network surveillance and preserve your personal freedom and privacy on the Internet. [SEP] Donate [SEP] Tor Servers [SEP] We provide high-bandwidth Tor nodes all over the world to protect online privacy, anonymity, freedom of speech and fight censorship! [SEP] '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_string = ['[CLS]' + (' [SEP] ').join(txt) + ' [SEP] ' for txt in texts_set]\n",
    "texts_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8192b2df45149e2add49a4663a1bada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=760289.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained('albert-large-v2', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '▁organization',\n",
       " '[SEP]',\n",
       " '▁partners',\n",
       " '[SEP]',\n",
       " '▁services',\n",
       " '[SEP]',\n",
       " '▁join',\n",
       " '▁us',\n",
       " '[SEP]',\n",
       " '▁skip',\n",
       " '▁to',\n",
       " '▁main',\n",
       " '▁content',\n",
       " '[SEP]',\n",
       " '▁abuse',\n",
       " '[SEP]',\n",
       " '▁world',\n",
       " 'map',\n",
       " '[SEP]',\n",
       " '▁support',\n",
       " '[SEP]',\n",
       " '▁donation',\n",
       " '▁history',\n",
       " '[SEP]',\n",
       " '▁become',\n",
       " '▁a',\n",
       " '▁member',\n",
       " '[SEP]',\n",
       " '▁mirrors',\n",
       " '[SEP]',\n",
       " '▁about',\n",
       " '[SEP]',\n",
       " '▁learn',\n",
       " '▁more',\n",
       " '▁about',\n",
       " '▁to',\n",
       " 'r',\n",
       " '[SEP]',\n",
       " '▁mail',\n",
       " 'ing',\n",
       " '▁list',\n",
       " '[SEP]',\n",
       " '▁fren',\n",
       " 'n',\n",
       " '▁v',\n",
       " 'un',\n",
       " '▁der',\n",
       " '▁en',\n",
       " 'n',\n",
       " '▁',\n",
       " '-',\n",
       " '▁index',\n",
       " '[SEP]',\n",
       " '▁fren',\n",
       " 'n',\n",
       " '▁v',\n",
       " 'un',\n",
       " '▁der',\n",
       " '▁en',\n",
       " 'n',\n",
       " '▁a',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'b',\n",
       " '.',\n",
       " 'l',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '▁',\n",
       " '@',\n",
       " 'f',\n",
       " 'ren',\n",
       " 'nv',\n",
       " 'under',\n",
       " 'en',\n",
       " 'n',\n",
       " '[SEP]',\n",
       " '▁',\n",
       " 'wiki',\n",
       " '[SEP]',\n",
       " '▁adopt',\n",
       " '▁a',\n",
       " '▁bridge',\n",
       " '[SEP]',\n",
       " '▁twitter',\n",
       " '[SEP]',\n",
       " '▁privacy',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁right',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁free',\n",
       " '▁mind',\n",
       " '!',\n",
       " '[SEP]',\n",
       " '▁statistics',\n",
       " '[SEP]',\n",
       " '▁i',\n",
       " '▁support',\n",
       " '▁f',\n",
       " 'v',\n",
       " 'de',\n",
       " '▁because',\n",
       " '▁even',\n",
       " '▁small',\n",
       " '▁organizations',\n",
       " '▁can',\n",
       " '▁contribute',\n",
       " '▁to',\n",
       " '▁protect',\n",
       " '▁fundamental',\n",
       " '▁civil',\n",
       " '▁rights',\n",
       " '.',\n",
       " '▁this',\n",
       " '▁noble',\n",
       " '▁goal',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁pursued',\n",
       " '▁by',\n",
       " '▁making',\n",
       " '▁use',\n",
       " '▁of',\n",
       " '▁technology',\n",
       " '▁and',\n",
       " '▁putting',\n",
       " '▁knowledge',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁service',\n",
       " '▁of',\n",
       " '▁users',\n",
       " '▁who',\n",
       " '▁want',\n",
       " '▁to',\n",
       " '▁maintain',\n",
       " '▁their',\n",
       " '▁privacy',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '▁georges',\n",
       " ',',\n",
       " '▁teacher',\n",
       " '[SEP]',\n",
       " '▁traffic',\n",
       " '[SEP]',\n",
       " '▁general',\n",
       " '[SEP]',\n",
       " '▁en',\n",
       " 'n',\n",
       " '.',\n",
       " 'lu',\n",
       " '[SEP]',\n",
       " '▁cc',\n",
       " '-',\n",
       " 'by',\n",
       " '-',\n",
       " 'nc',\n",
       " '-',\n",
       " 'sa',\n",
       " '[SEP]',\n",
       " '▁news',\n",
       " '[SEP]',\n",
       " '▁luxembourg',\n",
       " '▁based',\n",
       " '▁non',\n",
       " '-',\n",
       " 'profit',\n",
       " '▁organization',\n",
       " '▁defending',\n",
       " '▁civil',\n",
       " '▁rights',\n",
       " '▁on',\n",
       " '▁the',\n",
       " '▁internet',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '▁fren',\n",
       " 'n',\n",
       " '▁v',\n",
       " 'un',\n",
       " '▁der',\n",
       " '▁en',\n",
       " 'n',\n",
       " '▁a',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'b',\n",
       " '.',\n",
       " 'l',\n",
       " '.',\n",
       " '▁',\n",
       " '(',\n",
       " 'r',\n",
       " '.',\n",
       " 'c',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " '▁luxembourg',\n",
       " '▁f',\n",
       " '▁9',\n",
       " '.',\n",
       " '47',\n",
       " '8',\n",
       " ')',\n",
       " '[SEP]',\n",
       " '▁about',\n",
       " '▁en',\n",
       " 'n',\n",
       " 'stat',\n",
       " 'us',\n",
       " '[SEP]',\n",
       " '▁edward',\n",
       " '▁j',\n",
       " '.',\n",
       " '▁snow',\n",
       " 'den',\n",
       " '[SEP]',\n",
       " '▁contact',\n",
       " '[SEP]',\n",
       " '▁by',\n",
       " '▁bouncing',\n",
       " '▁your',\n",
       " '▁communications',\n",
       " '▁around',\n",
       " '▁a',\n",
       " '▁distributed',\n",
       " '▁network',\n",
       " '▁of',\n",
       " '▁relay',\n",
       " 's',\n",
       " '▁run',\n",
       " '▁by',\n",
       " '▁volunteers',\n",
       " '▁all',\n",
       " '▁around',\n",
       " '▁the',\n",
       " '▁world',\n",
       " ',',\n",
       " '▁it',\n",
       " '▁prevent',\n",
       " 's',\n",
       " '▁somebody',\n",
       " '▁watching',\n",
       " '▁your',\n",
       " '▁internet',\n",
       " '▁connection',\n",
       " '▁from',\n",
       " '▁learning',\n",
       " '▁what',\n",
       " '▁sites',\n",
       " '▁you',\n",
       " '▁visit',\n",
       " ',',\n",
       " '▁and',\n",
       " '▁it',\n",
       " '▁prevent',\n",
       " 's',\n",
       " '▁the',\n",
       " '▁sites',\n",
       " '▁you',\n",
       " '▁visit',\n",
       " '▁from',\n",
       " '▁learning',\n",
       " '▁your',\n",
       " '▁physical',\n",
       " '▁location',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '▁to',\n",
       " 'r',\n",
       " '▁is',\n",
       " '▁free',\n",
       " '▁software',\n",
       " '▁and',\n",
       " '▁an',\n",
       " '▁open',\n",
       " '▁network',\n",
       " '▁that',\n",
       " '▁helps',\n",
       " '▁you',\n",
       " '▁defend',\n",
       " '▁against',\n",
       " '▁network',\n",
       " '▁surveillance',\n",
       " '▁and',\n",
       " '▁preserve',\n",
       " '▁your',\n",
       " '▁personal',\n",
       " '▁freedom',\n",
       " '▁and',\n",
       " '▁privacy',\n",
       " '▁on',\n",
       " '▁the',\n",
       " '▁internet',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '▁donate',\n",
       " '[SEP]',\n",
       " '▁to',\n",
       " 'r',\n",
       " '▁servers',\n",
       " '[SEP]',\n",
       " '▁we',\n",
       " '▁provide',\n",
       " '▁high',\n",
       " '-',\n",
       " 'band',\n",
       " 'wi',\n",
       " 'd',\n",
       " 'th',\n",
       " '▁to',\n",
       " 'r',\n",
       " '▁nodes',\n",
       " '▁all',\n",
       " '▁over',\n",
       " '▁the',\n",
       " '▁world',\n",
       " '▁to',\n",
       " '▁protect',\n",
       " '▁online',\n",
       " '▁privacy',\n",
       " ',',\n",
       " '▁an',\n",
       " 'onym',\n",
       " 'ity',\n",
       " ',',\n",
       " '▁freedom',\n",
       " '▁of',\n",
       " '▁speech',\n",
       " '▁and',\n",
       " '▁fight',\n",
       " '▁censorship',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(txt) for txt in texts_string]\n",
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251403\n",
      "3\n",
      "2196.808157099698\n"
     ]
    }
   ],
   "source": [
    "# Explore the lengths of body texts\n",
    "tokens_len = [len(txt) for txt in tokenized_texts]\n",
    "print(max(tokens_len))\n",
    "print(min(tokens_len))\n",
    "print(sum(tokens_len)/len(tokens_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length.\n",
    "# MAX_LEN = 128\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,  1165,     3, ...,     0,     0,     0],\n",
       "       [    2,    95,    50, ...,   878, 21012,   676],\n",
       "       [    2, 13447,    13, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2,  7582,  2109, ...,     0,     0,     0],\n",
       "       [    2,  1302,    91, ...,    10,  6664,  2032],\n",
       "       [    2,    13,     8, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the tokenizer to convert the tokens to their index numbers in the vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1986, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "# batch_size = 32\n",
    "batch_size = 128\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491d8fb5693141cbad505893dc4cbfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=685.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14543d8e299640cfbcc1ffae880b847b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=71509304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-large-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (ffn_output): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load AlbertForSequenceClassification, the pretrained ALBERT model with a single linear classification layer on top. \n",
    "model = AlbertForSequenceClassification.from_pretrained(\"albert-large-v2\", num_labels=8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8b08ddca9dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         outputs = self.albert(\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         )\n\u001b[0;32m--> 703\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mgroup_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             layer_group_output = self.albert_layer_groups[group_idx](\n\u001b[0m\u001b[1;32m    460\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malbert_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malbert_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malbert_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     ):\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         ffn_output = apply_chunking_to_forward(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/albert/modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # Training\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = output[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
